services:
 spark:
   build:
     context: .
     dockerfile: Dockerfile.spark
   environment:
     - SPARK_MODE=master
     - PYSPARK_PYTHON=python3.9
     - PYSPARK_DRIVER_PYTHON=python3.9
     - SPARK_RPC_AUTHENTICATION_ENABLED=no
     - SPARK_RPC_ENCRYPTION_ENABLED=no
     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
     - SPARK_SSL_ENABLED=no
     - MINIO_ENDPOINT=http://minio:9000
     - MINIO_ACCESS_KEY=minio
     - MINIO_SECRET_KEY=minioAdmin
   ports:
     - '8080:8080'
     - '7077:7077'
   volumes:
     - ./jars:/opt/airflow/jars
   networks:
     - airflow-spark-network


 spark-worker-1:
   build:
     context: .
     dockerfile: Dockerfile.spark
   environment:
     - SPARK_MODE=worker
     - PYSPARK_PYTHON=python3.9
     - PYSPARK_DRIVER_PYTHON=python3.9
     - SPARK_MASTER_URL=spark://spark:7077
     - SPARK_WORKER_MEMORY=1G  # Aumento de memoria para el worker
     - SPARK_WORKER_CORES=2  # Aumento de nÃºcleos para el worker
     - SPARK_RPC_AUTHENTICATION_ENABLED=no
     - SPARK_RPC_ENCRYPTION_ENABLED=no
     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
     - SPARK_SSL_ENABLED=no
     - MINIO_ENDPOINT=http://minio:9000
     - MINIO_ACCESS_KEY=minio
     - MINIO_SECRET_KEY=minioAdmin
   volumes:
     - ./jars:/opt/airflow/jars
   depends_on:
     - spark
   networks:
     - airflow-spark-network


   # Add MinIO service
 minio:
   image: minio/minio
   ports:
     - "9000:9000"
     - "9001:9001"
   environment:
     - MINIO_ACCESS_KEY=minio
     - MINIO_SECRET_KEY=minioAdmin
   command: server /data --console-address ":9001"
   volumes:
     - minio-data:/data
   networks:
     - airflow-spark-network
   healthcheck:
     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
     interval: 30s
     timeout: 20s
     retries: 3


 createbuckets:
   image: minio/mc
   depends_on:
     minio:
       condition: service_healthy
   networks:
     - airflow-spark-network
   entrypoint: >
     /bin/sh -c "
     sleep 5;
     mc config host add myminio http://minio:9000 minio minioAdmin;
     mc mb myminio/bronze;
     mc mb myminio/silver;
     mc mb myminio/gold;
     exit 0;
     "




 # Airflow services (updated with Python 3.9 base image)
 airflow-webserver:
   build:
     context: .
     dockerfile: Dockerfile.airflow  # New Airflow Dockerfile
   command: webserver
   environment:
     - PYSPARK_PYTHON=python3.9
     - PYSPARK_DRIVER_PYTHON=python3.9
     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
     - AIRFLOW__CORE__LOAD_EXAMPLES=false
     - AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here
     - AIRFLOW__PROVIDERS__APACHE_SPARK__SPARK_DEFAULT_CONN_ID=spark_default
     - AIRFLOW__PROVIDERS__APACHE_SPARK__SPARK_DEFAULT_CONN_URL=spark://spark:7077
     - MINIO_ENDPOINT=http://minio:9000
     - MINIO_ACCESS_KEY=minio
     - MINIO_SECRET_KEY=minioAdmin
   ports:
     - "8081:8080"
   volumes:
     - ./dags:/opt/airflow/dags
     - ./logs:/opt/airflow/logs
     - ./plugins:/opt/airflow/plugins
     - ./jars:/opt/airflow/jars
   depends_on:
     - airflow-init
   networks:
     - airflow-spark-network


 airflow-scheduler:
   build:
     context: .
     dockerfile: Dockerfile.airflow  # Same custom Airflow image
   command: scheduler
   environment:
     - PYSPARK_PYTHON=python3.9
     - PYSPARK_DRIVER_PYTHON=python3.9
     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
     - AIRFLOW__CORE__LOAD_EXAMPLES=false
     - AIRFLOW__PROVIDERS__APACHE_SPARK__SPARK_DEFAULT_CONN_ID=spark_default
     - AIRFLOW__PROVIDERS__APACHE_SPARK__SPARK_DEFAULT_CONN_URL=spark://spark:7077
     - MINIO_ENDPOINT=http://minio:9000
     - MINIO_ACCESS_KEY=minio
     - MINIO_SECRET_KEY=minioAdmin  
   volumes:
     - ./dags:/opt/airflow/dags
     - ./logs:/opt/airflow/logs
     - ./plugins:/opt/airflow/plugins
     - ./jars:/opt/airflow/jars
   depends_on:
     - airflow-init
   networks:
     - airflow-spark-network


 # Postgres remains unchanged
 postgres:
   image: postgres:13
   environment:
     - POSTGRES_USER=airflow
     - POSTGRES_PASSWORD=airflow
     - POSTGRES_DB=airflow
   volumes:
     - postgres-db-volume:/var/lib/postgresql/data
   networks:
     - airflow-spark-network
   healthcheck:
     test: ["CMD", "pg_isready", "-U", "airflow"]
     interval: 5s
     retries: 5


 airflow-init:
   build:
     context: .
     dockerfile: Dockerfile.airflow  # Same custom Airflow image
   entrypoint: /bin/bash
   command:
     - -c
     - |
       airflow db init &&
       airflow users create \
         --username airflow \
         --firstname Saad \
         --lastname El Bekkali \
         --role Admin \
         --email admin@example.com \
         --password airflow
   environment:
     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
     - AIRFLOW__CORE__LOAD_EXAMPLES=false
   depends_on:
     postgres:
       condition: service_healthy
   networks:
     - airflow-spark-network
  




networks:
 airflow-spark-network:
   driver: bridge


volumes:
 postgres-db-volume:
 minio-data:  